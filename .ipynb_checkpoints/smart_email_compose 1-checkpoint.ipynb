{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMPE257 Project1: E-mail Autocomplete Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yuhua He - Data preparation, preprocessing(tf-idf), K-means algorithm, Elbow method\n",
    "#### Yuanzhe Li - Finding business model, preprocessing(stemming), Testing\n",
    "#### Samuel Yang - Data prepraration, Documentation, preprocessing(feature extraction), Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What story is the data telling us?\n",
    "\n",
    "#### In this group assignment, the business objective we are focusing on, is the \"Smart Email Compose\" project. We are using the Enron email dataset, which has the \"file\" column and \"message\" column, after servel feature extracting process, we can extract more features like \"file name\", \"message body\", from\", \"to\", \"message id\". So, the data is basically show what we need. It provides several thousands of messages body with completely different structure. For example, some of the messages greeting messgae that comes up with the \"hello\" word frequently, and some don't. In order to recognize it, we try to apply the tf-tdf technique to find out the frequency of each words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What business problem can be solved using the data?\n",
    "\n",
    "#### From what we do and trying to processing the data, \"Content Discovery\" can be solved easily. After we appliedK-means cluserting algorithm, we can divided all the data into several clusters where has similar words. More importantly, by applying machine-learning algorithm, we can further categorized emails into 4 major groups, which is am essential observation for our project. We also apply the eblow method to interprete and validate the consistency within cluster analysis designed to find appropriate amount of clusters we need in this situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Library\n",
    "\n",
    "import email, re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import  LatentDirichletAllocation\n",
    "from scipy.spatial.distance import cdist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6325, 5) (10000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Read the data into a DataFrame\n",
    "emails_enrich_df = pd.read_csv('rdany_conversations_2016-03-01.csv')\n",
    "emails_df = pd.read_csv('emails.csv',nrows = 10000, error_bad_lines = False)\n",
    "print(emails_enrich_df.shape, emails_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>hashed_message_id</th>\n",
       "      <th>hashed_chat_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human</td>\n",
       "      <td>[START]</td>\n",
       "      <td>0</td>\n",
       "      <td>fce1649c457fcd02600f0f1aece46c1a88d3258fdfc2d8...</td>\n",
       "      <td>b71a7a6e3eae414113fd7c6d3fc3e13b50fd5fbf335b95...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>robot</td>\n",
       "      <td>Hi there, how are you!? 游때游때</td>\n",
       "      <td>23789</td>\n",
       "      <td>8601139615d43330986f3f27b78bcf0212d2a4f1e4a4d7...</td>\n",
       "      <td>b71a7a6e3eae414113fd7c6d3fc3e13b50fd5fbf335b95...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>human</td>\n",
       "      <td>Oh, thanks! I'm fine. This is an evening in my...</td>\n",
       "      <td>41177</td>\n",
       "      <td>a2eb7031aa536c89231517fc71ea3315c89ca50fef155b...</td>\n",
       "      <td>b71a7a6e3eae414113fd7c6d3fc3e13b50fd5fbf335b95...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robot</td>\n",
       "      <td>游땏 here is afternoon!</td>\n",
       "      <td>41598</td>\n",
       "      <td>e0ac851c9ff34cd39a7e490fb9021163b8be6df097737f...</td>\n",
       "      <td>b71a7a6e3eae414113fd7c6d3fc3e13b50fd5fbf335b95...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>human</td>\n",
       "      <td>How do you feel today? Tell me something about...</td>\n",
       "      <td>41629</td>\n",
       "      <td>0d2937ef1d5e806d46b78f2264bee44f9cbad0d68e4ce2...</td>\n",
       "      <td>b71a7a6e3eae414113fd7c6d3fc3e13b50fd5fbf335b95...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source                                               text   date  \\\n",
       "0  human                                            [START]      0   \n",
       "1  robot                         Hi there, how are you!? 游때游때  23789   \n",
       "2  human  Oh, thanks! I'm fine. This is an evening in my...  41177   \n",
       "3  robot                               游땏 here is afternoon!  41598   \n",
       "4  human  How do you feel today? Tell me something about...  41629   \n",
       "\n",
       "                                   hashed_message_id  \\\n",
       "0  fce1649c457fcd02600f0f1aece46c1a88d3258fdfc2d8...   \n",
       "1  8601139615d43330986f3f27b78bcf0212d2a4f1e4a4d7...   \n",
       "2  a2eb7031aa536c89231517fc71ea3315c89ca50fef155b...   \n",
       "3  e0ac851c9ff34cd39a7e490fb9021163b8be6df097737f...   \n",
       "4  0d2937ef1d5e806d46b78f2264bee44f9cbad0d68e4ce2...   \n",
       "\n",
       "                                      hashed_chat_id  \n",
       "0  b71a7a6e3eae414113fd7c6d3fc3e13b50fd5fbf335b95...  \n",
       "1  b71a7a6e3eae414113fd7c6d3fc3e13b50fd5fbf335b95...  \n",
       "2  b71a7a6e3eae414113fd7c6d3fc3e13b50fd5fbf335b95...  \n",
       "3  b71a7a6e3eae414113fd7c6d3fc3e13b50fd5fbf335b95...  \n",
       "4  b71a7a6e3eae414113fd7c6d3fc3e13b50fd5fbf335b95...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_enrich_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "def get_email_text(email):\n",
    "    '''To get the content from email objects'''\n",
    "    email_contents = []\n",
    "    for email_object in email.walk():\n",
    "        if email_object.get_content_type() == 'text/plain':\n",
    "            email_contents.append(email_object.get_payload())\n",
    "    return ''.join(email_contents)\n",
    "\n",
    "def split_email_addresses(line):\n",
    "    '''To separate multiple email addresses'''\n",
    "    if line:\n",
    "        addrs = line.split(',')\n",
    "        addrs = frozenset(map(lambda x: x.strip(), addrs))\n",
    "    else:\n",
    "        addrs = None\n",
    "    return addrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the emails into a list email objects\n",
    "messages = list(map(email.message_from_string, emails_df['message']))\n",
    "emails_df.drop('message', axis=1, inplace=True)\n",
    "\n",
    "# Get fields from parsed email objects\n",
    "keys = messages[0].keys()\n",
    "for key in keys:\n",
    "    emails_df[key] = [doc[key] for doc in messages]\n",
    "\n",
    "# Parse content from emails\n",
    "emails_df['content'] = list(map(get_email_text, messages))\n",
    "\n",
    "#rename\n",
    "df = emails_enrich_df.rename(index = str, columns = {\"text\" : \"content\"})\n",
    "\n",
    "enriched_df = emails_df['content'].append(df['content'], ignore_index = True)\n",
    "\n",
    "# Split multiple email addresses\n",
    "emails_df['From'] = emails_df['From'].map(split_email_addresses)\n",
    "emails_df['To'] = emails_df['To'].map(split_email_addresses)\n",
    "\n",
    "# Set index and drop columns with two few values\n",
    "emails_df = emails_df.set_index('Message-ID')\\\n",
    "    .drop(['file', 'Mime-Version', 'Content-Type', 'Content-Transfer-Encoding'], axis=1)\n",
    "\n",
    "# Parse datetime\n",
    "emails_df['Date'] = pd.to_datetime(emails_df['Date'], infer_datetime_format=True)\n",
    "\n",
    "#data_cleaning\n",
    "enriched_df = enriched_df[~enriched_df.index.duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            Here is our forecast\\n\\n \n",
       "1    Traveling to have a business meeting takes the...\n",
       "2                       test successful.  way to go!!!\n",
       "3    Randy,\\n\\n Can you send me a schedule of the s...\n",
       "4                  Let's shoot for Tuesday at 11:45.  \n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(enriched_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0000',\n",
       " '000000',\n",
       " '00000000',\n",
       " '000000000000935',\n",
       " '000000000001282',\n",
       " '000000000002507',\n",
       " '000000000009659',\n",
       " '000000000021442',\n",
       " '000000000041547',\n",
       " '000000000067320',\n",
       " '000000000069328',\n",
       " '000000000070920',\n",
       " '000000000076886',\n",
       " '000000000077129',\n",
       " '000000000077131',\n",
       " '000000000079824',\n",
       " '000000000079825',\n",
       " '00000788265',\n",
       " '0000105886',\n",
       " '000066',\n",
       " '0000ff',\n",
       " '0001',\n",
       " '000119',\n",
       " '0001404319',\n",
       " '0001pt',\n",
       " '00021579',\n",
       " '0004',\n",
       " '0005',\n",
       " '0006',\n",
       " '000711',\n",
       " '000714',\n",
       " '000817',\n",
       " '000822a',\n",
       " '000825',\n",
       " '000825noticeofbogspecialmtg',\n",
       " '0009',\n",
       " '000btu',\n",
       " '000c05a6',\n",
       " '000mmbtu',\n",
       " '001',\n",
       " '0010',\n",
       " '0011',\n",
       " '001nevereditthisline002',\n",
       " '002',\n",
       " '0025',\n",
       " '0026',\n",
       " '002_bull_001',\n",
       " '002f',\n",
       " '003',\n",
       " '0030',\n",
       " '0033',\n",
       " '003366',\n",
       " '003399',\n",
       " '0033cc',\n",
       " '0034',\n",
       " '004',\n",
       " '0040',\n",
       " '004180',\n",
       " '0044',\n",
       " '0047',\n",
       " '0048',\n",
       " '005',\n",
       " '0050',\n",
       " '00535fa2',\n",
       " '0054',\n",
       " '00545dbc',\n",
       " '0057',\n",
       " '006',\n",
       " '0063',\n",
       " '006666',\n",
       " '007',\n",
       " '0073',\n",
       " '007341',\n",
       " '0075',\n",
       " '0079',\n",
       " '008',\n",
       " '0080',\n",
       " '0080ff',\n",
       " '009',\n",
       " '0090',\n",
       " '009754f305d7e55bcfb1e94e',\n",
       " '0098',\n",
       " '00a',\n",
       " '00am',\n",
       " '00ccff',\n",
       " '00cst',\n",
       " '00est',\n",
       " '00ffff',\n",
       " '00ish',\n",
       " '00p',\n",
       " '00pm',\n",
       " '00q4',\n",
       " '00us',\n",
       " '00vom',\n",
       " '00z',\n",
       " '01',\n",
       " '010',\n",
       " '0100',\n",
       " '0101',\n",
       " '010129_audit_index',\n",
       " '010821szasz',\n",
       " '0109',\n",
       " '011',\n",
       " '011022',\n",
       " '011022gurantz',\n",
       " '011126higgs',\n",
       " '01121094',\n",
       " '01121761',\n",
       " '01121913',\n",
       " '01122168',\n",
       " '0114',\n",
       " '011442077834188',\n",
       " '0117',\n",
       " '0118',\n",
       " '012',\n",
       " '012000m9puc9p',\n",
       " '012001',\n",
       " '0122',\n",
       " '0125',\n",
       " '0127',\n",
       " '012902release',\n",
       " '013',\n",
       " '0134',\n",
       " '0136',\n",
       " '013f',\n",
       " '014',\n",
       " '0141',\n",
       " '0143',\n",
       " '0146',\n",
       " '015',\n",
       " '0156',\n",
       " '016',\n",
       " '01624',\n",
       " '01642',\n",
       " '0165',\n",
       " '01683f',\n",
       " '0169',\n",
       " '01742',\n",
       " '0175',\n",
       " '0177',\n",
       " '0179',\n",
       " '018',\n",
       " '0184',\n",
       " '01875',\n",
       " '019',\n",
       " '0194',\n",
       " '0196',\n",
       " '01a45',\n",
       " '01am',\n",
       " '01bn',\n",
       " '01cen906',\n",
       " '01lavaugcolagenda1',\n",
       " '01nodriv',\n",
       " '01pm',\n",
       " '01q1',\n",
       " '01settlement',\n",
       " '01v',\n",
       " '02',\n",
       " '020',\n",
       " '0200',\n",
       " '0201',\n",
       " '0201414',\n",
       " '0201416',\n",
       " '020168809',\n",
       " '020169166',\n",
       " '0202112',\n",
       " '0202142',\n",
       " '0202437',\n",
       " '0203094',\n",
       " '0204',\n",
       " '0206',\n",
       " '0207',\n",
       " '02078234646',\n",
       " '021',\n",
       " '02109',\n",
       " '0211',\n",
       " '02110',\n",
       " '02138',\n",
       " '0215',\n",
       " '022',\n",
       " '0225',\n",
       " '023',\n",
       " '0232959',\n",
       " '0235',\n",
       " '024',\n",
       " '0245',\n",
       " '0246',\n",
       " '025',\n",
       " '0254',\n",
       " '0258',\n",
       " '026',\n",
       " '0260',\n",
       " '0265',\n",
       " '027',\n",
       " '0275',\n",
       " '028',\n",
       " '0281',\n",
       " '029',\n",
       " '0292',\n",
       " '0295',\n",
       " '02_',\n",
       " '02am',\n",
       " '02cen001',\n",
       " '02p',\n",
       " '02pm',\n",
       " '03',\n",
       " '030',\n",
       " '0300',\n",
       " '03037b',\n",
       " '0305',\n",
       " '0306',\n",
       " '030602daily',\n",
       " '0308',\n",
       " '0311',\n",
       " '0312072a',\n",
       " '03125',\n",
       " '03151',\n",
       " '031601mara',\n",
       " '0322',\n",
       " '0329',\n",
       " '0330672',\n",
       " '0339',\n",
       " '0341',\n",
       " '035',\n",
       " '0356',\n",
       " '0357',\n",
       " '0364',\n",
       " '0365',\n",
       " '0366',\n",
       " '0368',\n",
       " '0375',\n",
       " '03750',\n",
       " '0380',\n",
       " '0384',\n",
       " '0397',\n",
       " '03am',\n",
       " '03pm',\n",
       " '04',\n",
       " '040',\n",
       " '0400',\n",
       " '041',\n",
       " '0413',\n",
       " '042',\n",
       " '0431',\n",
       " '044',\n",
       " '0440',\n",
       " '0441',\n",
       " '0444',\n",
       " '0447',\n",
       " '0449',\n",
       " '045',\n",
       " '0450',\n",
       " '046',\n",
       " '0475',\n",
       " '048',\n",
       " '0481',\n",
       " '0493',\n",
       " '04hr',\n",
       " '05',\n",
       " '050',\n",
       " '0500',\n",
       " '05055',\n",
       " '05075',\n",
       " '051',\n",
       " '0511',\n",
       " '0518e',\n",
       " '052',\n",
       " '0520',\n",
       " '0521',\n",
       " '0525',\n",
       " '0528',\n",
       " '053',\n",
       " '0530',\n",
       " '0532',\n",
       " '0533m',\n",
       " '054',\n",
       " '0540',\n",
       " '0543',\n",
       " '055',\n",
       " '0550',\n",
       " '0558',\n",
       " '056',\n",
       " '0560',\n",
       " '05625',\n",
       " '0570',\n",
       " '058',\n",
       " '0582',\n",
       " '05848',\n",
       " '05852',\n",
       " '059',\n",
       " '0593',\n",
       " '0597',\n",
       " '05990',\n",
       " '05am',\n",
       " '05p',\n",
       " '05pm',\n",
       " '05s',\n",
       " '06',\n",
       " '0600',\n",
       " '06019',\n",
       " '060500',\n",
       " '0606',\n",
       " '061',\n",
       " '0612',\n",
       " '0614',\n",
       " '062000',\n",
       " '062301',\n",
       " '0625',\n",
       " '063',\n",
       " '064',\n",
       " '0643',\n",
       " '065',\n",
       " '0665',\n",
       " '06716',\n",
       " '0674',\n",
       " '0675',\n",
       " '0679',\n",
       " '068',\n",
       " '06853',\n",
       " '0686',\n",
       " '06897',\n",
       " '069',\n",
       " '0691',\n",
       " '0699',\n",
       " '06am',\n",
       " '06mar',\n",
       " '06nov',\n",
       " '07',\n",
       " '070',\n",
       " '0700',\n",
       " '07006',\n",
       " '0702',\n",
       " '071',\n",
       " '072800',\n",
       " '073',\n",
       " '07302',\n",
       " '07304',\n",
       " '0732',\n",
       " '074',\n",
       " '0742',\n",
       " '07452',\n",
       " '075',\n",
       " '0755',\n",
       " '076',\n",
       " '0765',\n",
       " '0768',\n",
       " '077',\n",
       " '0771',\n",
       " '0773',\n",
       " '0774',\n",
       " '07751',\n",
       " '079',\n",
       " '07am',\n",
       " '07e',\n",
       " '07nov',\n",
       " '07rabb',\n",
       " '08',\n",
       " '080',\n",
       " '0800',\n",
       " '0801',\n",
       " '0808',\n",
       " '0809',\n",
       " '081',\n",
       " '081301',\n",
       " '0817',\n",
       " '0821',\n",
       " '0822',\n",
       " '0829',\n",
       " '083',\n",
       " '0833',\n",
       " '0837',\n",
       " '084',\n",
       " '0840',\n",
       " '085',\n",
       " '0850',\n",
       " '0852',\n",
       " '0853',\n",
       " '0859',\n",
       " '086',\n",
       " '0862',\n",
       " '087',\n",
       " '0870',\n",
       " '0877',\n",
       " '0878',\n",
       " '088',\n",
       " '089',\n",
       " '08943a37',\n",
       " '08am',\n",
       " '08pm',\n",
       " '09',\n",
       " '090',\n",
       " '0900',\n",
       " '0901',\n",
       " '0908',\n",
       " '091',\n",
       " '0910',\n",
       " '0911',\n",
       " '0912',\n",
       " '09123',\n",
       " '0913',\n",
       " '0913259',\n",
       " '0913873',\n",
       " '0914',\n",
       " '0914236',\n",
       " '0915',\n",
       " '0916',\n",
       " '0917',\n",
       " '0918',\n",
       " '0919',\n",
       " '092',\n",
       " '0920',\n",
       " '092001',\n",
       " '0923',\n",
       " '0924',\n",
       " '0926',\n",
       " '0928',\n",
       " '093',\n",
       " '0931',\n",
       " '09320',\n",
       " '09365',\n",
       " '0937041',\n",
       " '09391',\n",
       " '094',\n",
       " '09415',\n",
       " '0946',\n",
       " '0949',\n",
       " '095',\n",
       " '0950',\n",
       " '0955',\n",
       " '0956',\n",
       " '0959',\n",
       " '096',\n",
       " '096042',\n",
       " '096064',\n",
       " '0962',\n",
       " '097',\n",
       " '09713',\n",
       " '0977',\n",
       " '098',\n",
       " '098010',\n",
       " '0981',\n",
       " '0981503886',\n",
       " '09824066305',\n",
       " '0985',\n",
       " '09853',\n",
       " '0989',\n",
       " '099',\n",
       " '09advertise',\n",
       " '09alex',\n",
       " '09allen',\n",
       " '09allison',\n",
       " '09alvarez',\n",
       " '09amex',\n",
       " '09an',\n",
       " '09angie',\n",
       " '09arg',\n",
       " '09armani',\n",
       " '09arnold',\n",
       " '09arora',\n",
       " '09as',\n",
       " '09associate',\n",
       " '09august',\n",
       " '09austr',\n",
       " '09australia',\n",
       " '09authorities',\n",
       " '09avista',\n",
       " '09bear',\n",
       " '09benchluch',\n",
       " '09bishops',\n",
       " '09black',\n",
       " '09both',\n",
       " '09boyd',\n",
       " '09brandi',\n",
       " '09braz',\n",
       " '09brief',\n",
       " '09brought',\n",
       " '09bruce',\n",
       " '09buzz',\n",
       " '09by',\n",
       " '09california',\n",
       " '09can',\n",
       " '09canada',\n",
       " '09capital',\n",
       " '09cathy',\n",
       " '09change',\n",
       " '09cheryl',\n",
       " '09cindy',\n",
       " '09click',\n",
       " '09close',\n",
       " '09coal',\n",
       " '09cob',\n",
       " '09colleen',\n",
       " '09commodities',\n",
       " '09complaint',\n",
       " '09conoco',\n",
       " '09contract',\n",
       " '09costs',\n",
       " '09cpuc',\n",
       " '09credit',\n",
       " '09crenshaw',\n",
       " '09crude',\n",
       " '09customers',\n",
       " '09daily',\n",
       " '09dailyquote',\n",
       " '09danilov',\n",
       " '09dave',\n",
       " '09dear',\n",
       " '09december',\n",
       " '09demand',\n",
       " '09distribution',\n",
       " '09dj',\n",
       " '09djia',\n",
       " '09dollar',\n",
       " '09dpl',\n",
       " '09eb5c2',\n",
       " '09editor',\n",
       " '09employees',\n",
       " '09energy',\n",
       " '09enron',\n",
       " '09espinoza',\n",
       " '09euro',\n",
       " '09fantasy',\n",
       " '09feedback',\n",
       " '09ferc',\n",
       " '09financial',\n",
       " '09florida',\n",
       " '09for',\n",
       " '09foreign',\n",
       " '09forster',\n",
       " '09friday',\n",
       " '09from',\n",
       " '09further',\n",
       " '09future',\n",
       " '09fw',\n",
       " '09fyi',\n",
       " '09gas',\n",
       " '09generators',\n",
       " '09germany',\n",
       " '09get',\n",
       " '09go',\n",
       " '09golf',\n",
       " '09good',\n",
       " '09greg',\n",
       " '09he',\n",
       " '09heating',\n",
       " '09help',\n",
       " '09hence',\n",
       " '09high',\n",
       " '09hk',\n",
       " '09i',\n",
       " '09identify',\n",
       " '09in',\n",
       " '09inside',\n",
       " '09issler',\n",
       " '09it',\n",
       " '09jacoby',\n",
       " '09james',\n",
       " '09japan',\n",
       " '09jarnold',\n",
       " '09jennifer',\n",
       " '09john',\n",
       " '09jp',\n",
       " '09julie',\n",
       " '09kerr',\n",
       " '09kristin',\n",
       " '09lawner',\n",
       " '09lehman',\n",
       " '09log',\n",
       " '09london',\n",
       " '09long',\n",
       " '09louise',\n",
       " '09low',\n",
       " '09m',\n",
       " '09mack',\n",
       " '09market',\n",
       " '09medium',\n",
       " '09message',\n",
       " '09mexico',\n",
       " '09min',\n",
       " '09mirant',\n",
       " '09models',\n",
       " '09mollusks',\n",
       " '09molly',\n",
       " '09monday',\n",
       " '09monthly',\n",
       " '09more',\n",
       " '09mother',\n",
       " '09nasdaq',\n",
       " '09nat',\n",
       " '09new',\n",
       " '09nicki',\n",
       " '09nicolay',\n",
       " '09nstar',\n",
       " '09nyse',\n",
       " '09october',\n",
       " '09of',\n",
       " '09oil',\n",
       " '09on',\n",
       " '09once',\n",
       " '09opec',\n",
       " '09options',\n",
       " '09otto',\n",
       " '09outlook',\n",
       " '09pallen',\n",
       " '09palo',\n",
       " '09paulo',\n",
       " '09perhaps',\n",
       " '09phillip',\n",
       " '09pira',\n",
       " '09pjm',\n",
       " '09plan',\n",
       " '09pmg',\n",
       " '09power',\n",
       " '09powerindex',\n",
       " '09presentation',\n",
       " '09price',\n",
       " '09primary',\n",
       " '09privacy',\n",
       " '09provide',\n",
       " '09quote',\n",
       " '09re',\n",
       " '09receipt',\n",
       " '09regional',\n",
       " '09return',\n",
       " '09s',\n",
       " '09saturday',\n",
       " '09schmidt',\n",
       " '09section',\n",
       " '09sent',\n",
       " '09shares',\n",
       " '09short',\n",
       " '09showtimes',\n",
       " '09site',\n",
       " '09some',\n",
       " '09source',\n",
       " '09sponsored',\n",
       " '09sqrt',\n",
       " '09staff',\n",
       " '09state',\n",
       " '09stay',\n",
       " '09stocks',\n",
       " '09tax',\n",
       " '09taxable',\n",
       " '09tennessee',\n",
       " '09texas',\n",
       " '09the',\n",
       " '09there',\n",
       " '09this',\n",
       " '09thursday',\n",
       " '09tips',\n",
       " '09to',\n",
       " '09tuesday',\n",
       " '09uk',\n",
       " '09updated',\n",
       " '09uses',\n",
       " '09utility',\n",
       " '09value',\n",
       " '09visitors',\n",
       " '09vol',\n",
       " '09volume',\n",
       " '09ward',\n",
       " '09wednesday',\n",
       " '09wesner',\n",
       " '09whalley',\n",
       " '09what',\n",
       " '09whether',\n",
       " '09while',\n",
       " '09whilst',\n",
       " '09whitt',\n",
       " '09word',\n",
       " '09wtd',\n",
       " '09yes',\n",
       " '09you',\n",
       " '0__',\n",
       " '0enron',\n",
       " '0f',\n",
       " '0in',\n",
       " '0m',\n",
       " '0mm',\n",
       " '0pm',\n",
       " '0pt',\n",
       " '0us',\n",
       " '0x46856181',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '100012',\n",
       " '10002661',\n",
       " '10002702',\n",
       " '10002713',\n",
       " '10002864',\n",
       " '10002964',\n",
       " '10002968',\n",
       " '10003035',\n",
       " '10003075',\n",
       " '10003083',\n",
       " '10003162',\n",
       " '10003164',\n",
       " '10003860',\n",
       " '1000mhz',\n",
       " '1000s',\n",
       " '1001',\n",
       " '10013',\n",
       " '10016',\n",
       " '10018',\n",
       " '10019',\n",
       " '1002',\n",
       " '10022',\n",
       " '10028',\n",
       " '100301',\n",
       " '10035',\n",
       " '1005',\n",
       " '10052',\n",
       " '1006237351',\n",
       " '1006237417',\n",
       " '1009',\n",
       " '10094',\n",
       " '1009852676',\n",
       " '100bn',\n",
       " '100bp',\n",
       " '100bps',\n",
       " '100k',\n",
       " '100m',\n",
       " '100mb',\n",
       " '100mw',\n",
       " '100s',\n",
       " '101',\n",
       " '1010',\n",
       " '10100',\n",
       " '1011',\n",
       " '10119',\n",
       " '1012',\n",
       " '1014',\n",
       " '1014433',\n",
       " '1015',\n",
       " '101501',\n",
       " '10153',\n",
       " '1017',\n",
       " '10176',\n",
       " '10178',\n",
       " '1018',\n",
       " '10181',\n",
       " '10192',\n",
       " '101a',\n",
       " '101b',\n",
       " '102',\n",
       " '10219',\n",
       " '1022',\n",
       " '10225',\n",
       " '102255',\n",
       " '1023',\n",
       " '102400salsa',\n",
       " '102401',\n",
       " '10243',\n",
       " '1025',\n",
       " '10251',\n",
       " '1025253',\n",
       " '1026',\n",
       " '10260',\n",
       " '102601bw',\n",
       " '102601k1',\n",
       " '10279',\n",
       " '10299',\n",
       " '103',\n",
       " '1030',\n",
       " '10300',\n",
       " '1031',\n",
       " '103237',\n",
       " '10335',\n",
       " '1034',\n",
       " '1037',\n",
       " '104',\n",
       " '10400',\n",
       " '10403',\n",
       " '10420',\n",
       " '10432',\n",
       " '10435',\n",
       " '10440',\n",
       " '104525',\n",
       " '10457',\n",
       " '1049',\n",
       " '104s',\n",
       " '105',\n",
       " '10501',\n",
       " '105081',\n",
       " '10520',\n",
       " '10527',\n",
       " '10545',\n",
       " '105465',\n",
       " '10555',\n",
       " '105654',\n",
       " '105801450',\n",
       " '105891',\n",
       " '105894',\n",
       " '105km',\n",
       " '105th',\n",
       " '106',\n",
       " '1060',\n",
       " '10600',\n",
       " '10601',\n",
       " '1061',\n",
       " '10612',\n",
       " '1063',\n",
       " '106300',\n",
       " '1069',\n",
       " '107',\n",
       " '1073',\n",
       " '10738',\n",
       " '107443',\n",
       " '1075',\n",
       " '10755',\n",
       " '10794',\n",
       " '108',\n",
       " '1085',\n",
       " '10861',\n",
       " '10880',\n",
       " '1089',\n",
       " '109',\n",
       " '1095',\n",
       " '1097',\n",
       " '1099',\n",
       " '109e3',\n",
       " '10am',\n",
       " '10b',\n",
       " '10bn',\n",
       " '10bps',\n",
       " '10gb',\n",
       " '10k',\n",
       " '10m',\n",
       " '10mar',\n",
       " '10mb',\n",
       " '10mm',\n",
       " '10p',\n",
       " '10pm',\n",
       " '10pt',\n",
       " '10px',\n",
       " '10q',\n",
       " '10qsb',\n",
       " '10s',\n",
       " '10th',\n",
       " '10y',\n",
       " '10yr',\n",
       " '10yrs',\n",
       " '11',\n",
       " '110',\n",
       " '1100',\n",
       " '11000015',\n",
       " '11008607',\n",
       " '11008650',\n",
       " '1100a',\n",
       " '1102',\n",
       " '110201',\n",
       " '1103',\n",
       " '1105',\n",
       " '1107435',\n",
       " '1108',\n",
       " '1109870',\n",
       " '110m',\n",
       " '111',\n",
       " '1110',\n",
       " '1111',\n",
       " '1112',\n",
       " '11121',\n",
       " '1112587',\n",
       " '1112596',\n",
       " '1115',\n",
       " '11158',\n",
       " '1116001128',\n",
       " '1118',\n",
       " '1119',\n",
       " '112',\n",
       " '1120',\n",
       " '112001',\n",
       " '11209',\n",
       " '1121',\n",
       " '11211',\n",
       " '1122',\n",
       " '1124',\n",
       " '1125',\n",
       " '1126',\n",
       " '1127',\n",
       " '11271',\n",
       " '1128',\n",
       " '113',\n",
       " '1130',\n",
       " '1130ish',\n",
       " '1132',\n",
       " '113314',\n",
       " '1135',\n",
       " '11359',\n",
       " '1136',\n",
       " '11377',\n",
       " '1138',\n",
       " '1139',\n",
       " '114',\n",
       " '1140',\n",
       " '11408',\n",
       " '1141',\n",
       " '1142',\n",
       " '1144',\n",
       " '1144byg',\n",
       " '1145',\n",
       " '1145a',\n",
       " '1146',\n",
       " '11466',\n",
       " '1147',\n",
       " '1149',\n",
       " '115',\n",
       " '1150',\n",
       " '1150540202173',\n",
       " '1151',\n",
       " '1151268',\n",
       " '1152',\n",
       " '1153',\n",
       " '1154',\n",
       " '1155',\n",
       " '11558',\n",
       " '1156',\n",
       " '1157',\n",
       " '1158',\n",
       " '1159',\n",
       " '115k',\n",
       " '116',\n",
       " '1160',\n",
       " '11604',\n",
       " '1160mw',\n",
       " '1161',\n",
       " '11613',\n",
       " '1162',\n",
       " '1163',\n",
       " '1164',\n",
       " '1165',\n",
       " '1166',\n",
       " '11673325',\n",
       " '1168',\n",
       " '1169',\n",
       " '11690x17742762',\n",
       " '11690x34851784',\n",
       " '11690x60348122',\n",
       " '11690x66791593',\n",
       " '11690x72542557',\n",
       " '117',\n",
       " '1170',\n",
       " '11711',\n",
       " '11723',\n",
       " '1173',\n",
       " '1175',\n",
       " '1178',\n",
       " '118',\n",
       " '118088',\n",
       " '11815',\n",
       " '1182',\n",
       " '1183',\n",
       " '1185',\n",
       " '11856',\n",
       " '11869',\n",
       " '11885',\n",
       " '1189',\n",
       " '119',\n",
       " '1193',\n",
       " '1194',\n",
       " '1195',\n",
       " '1199',\n",
       " '11_0',\n",
       " '11_00_autoquoteintro',\n",
       " '11a',\n",
       " '11am',\n",
       " '11b',\n",
       " '11bn',\n",
       " '11bps',\n",
       " '11c1',\n",
       " '11cap',\n",
       " '11d4',\n",
       " '11m',\n",
       " '11mbps',\n",
       " '11pm',\n",
       " '11px',\n",
       " '11th',\n",
       " '12',\n",
       " '120',\n",
       " '1200',\n",
       " '1201',\n",
       " '120101',\n",
       " '1203',\n",
       " '1205',\n",
       " '12069',\n",
       " '1208',\n",
       " '120800',\n",
       " '120800lennon',\n",
       " '1209',\n",
       " '120x800',\n",
       " '121',\n",
       " '1210',\n",
       " '1212',\n",
       " '1213c',\n",
       " '1214',\n",
       " '121401',\n",
       " ...]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#initialize the Vectorizer\n",
    "vect = CountVectorizer()\n",
    "# learn the vocab and parse them as features based on the given params.\n",
    "vect.fit(enriched_df.values)\n",
    "# get the feature names\n",
    "vect.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function - Removing stop words, removel unimpotant words.\n",
    "\n",
    "def clean(text):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stop.update(('fwd','RE','FW','Hello','Meeting','Ga','Access','positions','list','forward','floor','collar','fixed',\n",
    "    'enron','hou','ect','corp','please','vince','time','mail','john','kay','day','message','week','kaminski','year',\n",
    "    'meeting','enronxgate','question','group','work','call','scott','change','company','let','mann','date','number',\n",
    "    'mark','today','david','mike','issue','houston','chris','subject','way','bass','jeff','edu','office','doc','don',\n",
    "    'month','copy','name','comment','email','need','phone','point','thing','request','look','ben','michael','list',\n",
    "    'help','delainey','fax','morning','use','tomorrow','thank','phillip','hotmail','guy','robert','night','lon',\n",
    "    'part','talk','kate','home','mailto','person','address','form','jeffrey','something','end','line','hour',\n",
    "    'place','march','love','anything','paul','giron','smith','hope','darron','jim','kevin','weekend','george',\n",
    "    'north','someone','section','richard','discus','bob','jacoby','ena','room','see','demand','desk','area',\n",
    "    'everyone','greg','detail','jason','afternoon','discussion','tom','kslaw','check','basis','visit','mcconnell',\n",
    "    'miller','entity','location','peter','monday','response','show','page','jennifer','lot','meet','respond',\n",
    "    'yesterday','pdx','house','june','larry','jan','dan','city','july','judy','friday','julie','shirley','meter',\n",
    "    'level','fyi','addition','martin','anyone','generation','department','type','rick','friend','period','word',\n",
    "    'lisa','think','class','johnson','org','robin','thompson','columbiagas','didn','april','william','lee','thomas',\n",
    "    'hey','adam','stephen','man','sender','tim','taylor','organization','center','everything','ferc','notice',\n",
    "    'start','davis','york','sorry','cell','return','street','hernandez','thursday','campbell','care','content',\n",
    "    'curve','minute','floor','stinson','janet','head','move','kind','kent','tuesday','sheila','send','suzanne',\n",
    "    'brenda','kim','matter','fgt','carolyn','cindy','ccampbell','tell','fwd','crenshaw','baumbach','linda','side',\n",
    "    'clark','mind','hain','wharton','future','errol','carlos','hand','matt','bruce','gossett','brian','try',\n",
    "    'wednesday','calendar','laura','nothing','doug','llc','rebecca','rob','stephanie','austin','victor','join',\n",
    "    'joseph','couple', 'allen', 'kean', 'arnold', 'var', 'keith', 'lucy', 'grigsby','com', 'cc', 'pm', 'http',\n",
    "    'market','size', 'image', 'td', 'align', 'font','www', 'gas' ,'send', 'original', 'pt','tr','send','br','k','f','b' ))\n",
    "    exclude = set(string.punctuation) \n",
    "    lemma = WordNetLemmatizer()\n",
    "    \n",
    "    text=text.rstrip()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    stop_free = \" \".join([i for i in text.lower().split() if((i not in stop) and (not i.isdigit()))])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word, pos='v') for word in punc_free.split())\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning, removing stop words\n",
    "\n",
    "analysis_df=emails_df[['From', 'To', 'Date','content']].dropna().copy()\n",
    "analysis_df = analysis_df.loc[analysis_df['To'].map(len) == 1]\n",
    "analysis_df[\"clean_content\"]=analysis_df.content.apply(clean)\n",
    "analysis_df[\"clean_content\"].to_csv('clean_c.csv', index = False, header = True)\n",
    "analysis_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Data Before And After Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_simple = emails_df.content[0]\n",
    "print(content_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data After Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean(emails_df.content[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvector = TfidfVectorizer(analyzer='word', stop_words='english', max_df=0.4, min_df=5)\n",
    "short_analysis=analysis_df.sample(5000)\n",
    "wordvector_fit = wordvector.fit_transform(short_analysis.clean_content)\n",
    "feature = wordvector.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make elbow plot, find k\n",
    "distortions = []\n",
    "k_list = range(1, 5)\n",
    "for N in k_list:\n",
    "    clf = KMeans(n_clusters=N, \n",
    "            max_iter=50, \n",
    "            init='k-means++', \n",
    "            n_init=1)\n",
    "    labels = clf.fit_predict(wordvector_fit)\n",
    "    wordvector_fit_2d = wordvector_fit.todense()\n",
    "    distortions.append(sum(np.min(cdist(wordvector_fit_2d, clf.cluster_centers_, 'euclidean'), axis=1)) / wordvector_fit_2d.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(k_list, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering Algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N = 4\n",
    "clf = KMeans(n_clusters=N, \n",
    "            max_iter=50, \n",
    "            init='k-means++', \n",
    "            n_init=1)\n",
    "labels = clf.fit_predict(wordvector_fit)\n",
    "\n",
    "wordvector_fit_2d = wordvector_fit.todense()\n",
    "pca = PCA(n_components=2).fit(wordvector_fit_2d)\n",
    "datapoint = pca.transform(wordvector_fit_2d)\n",
    "\n",
    "label = [\"#e05f14\", \"#e0dc14\", \"#2fe014\", \"#14d2e0\"]\n",
    "color = [label[i] for i in labels]\n",
    "plt.scatter(datapoint[:, 0], datapoint[:, 1], c=color)\n",
    "\n",
    "centroids = clf.cluster_centers_\n",
    "centroidpoint = pca.transform(centroids)\n",
    "plt.scatter(centroidpoint[:, 0], centroidpoint[:, 1], marker='^', s=150, c='#000000')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New LDA\n",
    "text_clean=[]\n",
    "for text in short_analysis['clean_content']:\n",
    "    text_clean.append(text.split())\n",
    "dictionary = corpora.Dictionary(text_clean)\n",
    "text_term_matrix = [dictionary.doc2bow(text) for text in text_clean]\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(text_term_matrix, num_topics=10, id2word = dictionary, passes=30)\n",
    "topics = ldamodel.print_topics(num_topics=10, num_words=10)\n",
    "for i, j in enumerate(topics):\n",
    "    print(\"Topic: {}\\n{}\\n\".format(i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Old LDA\n",
    "features = 10\n",
    "topics = 10\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(analysis_df.clean_content)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "lda = LatentDirichletAllocation(n_components = topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print (\"Topic %d:\" % (topic_idx))\n",
    "    print (\" \".join([tf_feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixed Model (GMM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=4).fit(datapoint)\n",
    "labels = gmm.predict(datapoint)\n",
    "plt.scatter(datapoint[:, 0], datapoint[:, 1], c=labels, s=40, cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
