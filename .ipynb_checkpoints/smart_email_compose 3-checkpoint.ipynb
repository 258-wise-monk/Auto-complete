{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMPE257 Project1: E-mail Autocomplete Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yuhua He - Data preparation, preprocessing(tf-idf), K-means algorithm, Elbow method\n",
    "#### Yuanzhe Li - Finding business model, preprocessing(stemming), Testing\n",
    "#### Samuel Yang - Data prepraration, Documentation, preprocessing(feature extraction), Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What story is the data telling us?\n",
    "\n",
    "#### In this group assignment, the business objective we are focusing on, is the \"Smart Email Compose\" project. We are using the Enron email dataset, which has the \"file\" column and \"message\" column, after servel feature extracting process, we can extract more features like \"file name\", \"message body\", from\", \"to\", \"message id\". So, the data is basically show what we need. It provides several thousands of messages body with completely different structure. For example, some of the messages greeting messgae that comes up with the \"hello\" word frequently, and some don't. In order to recognize it, we try to apply the tf-tdf technique to find out the frequency of each words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What business problem can be solved using the data?\n",
    "\n",
    "#### From what we do and trying to processing the data, \"Content Discovery\" can be solved easily. After we appliedK-means cluserting algorithm, we can divided all the data into several clusters where has similar words. More importantly, by applying machine-learning algorithm, we can further categorized emails into 4 major groups, which is am essential observation for our project. We also apply the eblow method to interprete and validate the consistency within cluster analysis designed to find appropriate amount of clusters we need in this situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Library\n",
    "\n",
    "import email, re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import  LatentDirichletAllocation\n",
    "from scipy.spatial.distance import cdist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strong initial dataset (Enron Email Dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6325, 5) (10000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Read the data into a DataFrame\n",
    "emails_enrich_df = pd.read_csv('rdany_conversations_2016-03-01.csv')\n",
    "emails_df = pd.read_csv('emails.csv',nrows = 10000, error_bad_lines = False)\n",
    "print(emails_enrich_df.shape, emails_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Enrichment (Coversation Dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>hashed_message_id</th>\n",
       "      <th>hashed_chat_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human</td>\n",
       "      <td>[START]</td>\n",
       "      <td>0</td>\n",
       "      <td>fce1649c457fcd02600f0f1aece46c1a88d3258fdfc2d8...</td>\n",
       "      <td>b71a7a6e3eae414113fd7c6d3fc3e13b50fd5fbf335b95...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>robot</td>\n",
       "      <td>Hi there, how are you!? 游때游때</td>\n",
       "      <td>23789</td>\n",
       "      <td>8601139615d43330986f3f27b78bcf0212d2a4f1e4a4d7...</td>\n",
       "      <td>b71a7a6e3eae414113fd7c6d3fc3e13b50fd5fbf335b95...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>human</td>\n",
       "      <td>Oh, thanks! I'm fine. This is an evening in my...</td>\n",
       "      <td>41177</td>\n",
       "      <td>a2eb7031aa536c89231517fc71ea3315c89ca50fef155b...</td>\n",
       "      <td>b71a7a6e3eae414113fd7c6d3fc3e13b50fd5fbf335b95...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robot</td>\n",
       "      <td>游땏 here is afternoon!</td>\n",
       "      <td>41598</td>\n",
       "      <td>e0ac851c9ff34cd39a7e490fb9021163b8be6df097737f...</td>\n",
       "      <td>b71a7a6e3eae414113fd7c6d3fc3e13b50fd5fbf335b95...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>human</td>\n",
       "      <td>How do you feel today? Tell me something about...</td>\n",
       "      <td>41629</td>\n",
       "      <td>0d2937ef1d5e806d46b78f2264bee44f9cbad0d68e4ce2...</td>\n",
       "      <td>b71a7a6e3eae414113fd7c6d3fc3e13b50fd5fbf335b95...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source                                               text   date  \\\n",
       "0  human                                            [START]      0   \n",
       "1  robot                         Hi there, how are you!? 游때游때  23789   \n",
       "2  human  Oh, thanks! I'm fine. This is an evening in my...  41177   \n",
       "3  robot                               游땏 here is afternoon!  41598   \n",
       "4  human  How do you feel today? Tell me something about...  41629   \n",
       "\n",
       "                                   hashed_message_id  \\\n",
       "0  fce1649c457fcd02600f0f1aece46c1a88d3258fdfc2d8...   \n",
       "1  8601139615d43330986f3f27b78bcf0212d2a4f1e4a4d7...   \n",
       "2  a2eb7031aa536c89231517fc71ea3315c89ca50fef155b...   \n",
       "3  e0ac851c9ff34cd39a7e490fb9021163b8be6df097737f...   \n",
       "4  0d2937ef1d5e806d46b78f2264bee44f9cbad0d68e4ce2...   \n",
       "\n",
       "                                      hashed_chat_id  \n",
       "0  b71a7a6e3eae414113fd7c6d3fc3e13b50fd5fbf335b95...  \n",
       "1  b71a7a6e3eae414113fd7c6d3fc3e13b50fd5fbf335b95...  \n",
       "2  b71a7a6e3eae414113fd7c6d3fc3e13b50fd5fbf335b95...  \n",
       "3  b71a7a6e3eae414113fd7c6d3fc3e13b50fd5fbf335b95...  \n",
       "4  b71a7a6e3eae414113fd7c6d3fc3e13b50fd5fbf335b95...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_enrich_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "def get_email_text(email):\n",
    "    '''To get the content from email objects'''\n",
    "    email_contents = []\n",
    "    for email_object in email.walk():\n",
    "        if email_object.get_content_type() == 'text/plain':\n",
    "            email_contents.append(email_object.get_payload())\n",
    "    return ''.join(email_contents)\n",
    "\n",
    "def split_email_addresses(line):\n",
    "    '''To separate multiple email addresses'''\n",
    "    if line:\n",
    "        addrs = line.split(',')\n",
    "        addrs = frozenset(map(lambda x: x.strip(), addrs))\n",
    "    else:\n",
    "        addrs = None\n",
    "    return addrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the emails into a list email objects\n",
    "messages = list(map(email.message_from_string, emails_df['message']))\n",
    "emails_df.drop('message', axis=1, inplace=True)\n",
    "\n",
    "# Get fields from parsed email objects\n",
    "keys = messages[0].keys()\n",
    "for key in keys:\n",
    "    emails_df[key] = [doc[key] for doc in messages]\n",
    "\n",
    "# Parse content from emails\n",
    "emails_df['content'] = list(map(get_email_text, messages))\n",
    "\n",
    "#rename\n",
    "df = emails_enrich_df.rename(index = str, columns = {\"text\" : \"content\"})\n",
    "\n",
    "enriched_df = emails_df['content'].append(df['content'], ignore_index = True)\n",
    "\n",
    "# Split multiple email addresses\n",
    "emails_df['From'] = emails_df['From'].map(split_email_addresses)\n",
    "emails_df['To'] = emails_df['To'].map(split_email_addresses)\n",
    "\n",
    "# Set index and drop columns with two few values\n",
    "emails_df = emails_df.set_index('Message-ID')\\\n",
    "    .drop(['file', 'Mime-Version', 'Content-Type', 'Content-Transfer-Encoding'], axis=1)\n",
    "\n",
    "# Parse datetime\n",
    "emails_df['Date'] = pd.to_datetime(emails_df['Date'], infer_datetime_format=True)\n",
    "\n",
    "#data_cleaning\n",
    "enriched_df = enriched_df[~enriched_df.index.duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            Here is our forecast\\n\\n \n",
       "1    Traveling to have a business meeting takes the...\n",
       "2                       test successful.  way to go!!!\n",
       "3    Randy,\\n\\n Can you send me a schedule of the s...\n",
       "4                  Let's shoot for Tuesday at 11:45.  \n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(enriched_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW is a really useful model for the data-preparation process especially for text data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ability', 'able', 'about', 'above', 'access', 'according', 'account', 'action', 'active', 'add', 'added', 'addition', 'additional', 'address', 'after', 'afternoon', 'again', 'against', 'ago', 'agreed', 'agreement', 'ahead', 'air', 'align', 'all', 'allen', 'allow', 'along', 'already', 'also', 'alt', 'always', 'am', 'america', 'american', 'among', 'amount', 'an', 'analysis', 'analyst', 'analysts', 'and', 'andy', 'announced', 'another', 'answer', 'any', 'anyone', 'anything', 'aol', 'approval', 'april', 'are', 'area', 'arial', 'arnold', 'around', 'as', 'ask', 'asked', 'asp', 'assets', 'associated', 'at', 'attached', 'attachments', 'attend', 'august', 'austin', 'available', 'average', 'away', 'back', 'bailey', 'balance', 'bank', 'based', 'basis', 'be', 'because', 'been', 'before', 'being', 'believe', 'below', 'best', 'better', 'between', 'bid', 'big', 'bill', 'billion', 'bloomberg', 'bmc', 'board', 'bob', 'book', 'border', 'both', 'br']\n"
     ]
    }
   ],
   "source": [
    "#cleaning all numeric data\n",
    "enriched_df = enriched_df.replace('\\d+', 'NUM', regex=True)\n",
    "\n",
    "#initialize the Vectorizer\n",
    "count_vec = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
    "\n",
    "# learn the vocab and parse them as features based on the given params.\n",
    "count_occurs = count_vec.fit_transform(enriched_df)\n",
    "\n",
    "# printing first hundred features\n",
    "print(count_vec.get_feature_names()[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Word  Count\n",
      "368     here      1\n",
      "623      our      1\n",
      "429       is      1\n",
      "664    plant      0\n",
      "669       pm      0\n",
      "668     plus      0\n",
      "667   please      0\n",
      "666     play      0\n",
      "665   plants      0\n",
      "0    ability      0\n"
     ]
    }
   ],
   "source": [
    "count_occur_df = pd.DataFrame((count, word) for word, count in zip(count_occurs.toarray().tolist()[0], count_vec.get_feature_names()))\n",
    "\n",
    "count_occur_df.columns = ['Word', 'Count']\n",
    "count_occur_df.sort_values('Count', ascending=False, inplace=True)\n",
    "print(count_occur_df.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression (with BoW model):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to perform linear regression model, besides combining with BoW, we also need to use LabelEncoder to convert string to numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "X = le.fit(count_occur_df['Word']).transform(count_occur_df['Word'])\n",
    "y = count_occur_df['Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train[:, None],y_train)\n",
    "\n",
    "predictions = lm.predict(X_test[:, None])\n",
    "plt.scatter(y_test,predictions)\n",
    "#plt.plot(X_test[:, y_test, color='blue', linewidth=3)\n",
    "plt.plot(X_test, predictions,color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the graph above we can see the words basically will all be like the \"here\", \"is\", \"our\", etc. Those words comes out frequently as a comparison to other words, so it will be useful for us to making a predictions that those words will more likely to appear after several patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function - Removing stop words, removel unimpotant words.\n",
    "\n",
    "def clean(text):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stop.update(('fwd','RE','FW','Hello','Meeting','Ga','Access','positions','list','forward','floor','collar','fixed',\n",
    "    'enron','hou','ect','corp','please','vince','time','mail','john','kay','day','message','week','kaminski','year',\n",
    "    'meeting','enronxgate','question','group','work','call','scott','change','company','let','mann','date','number',\n",
    "    'mark','today','david','mike','issue','houston','chris','subject','way','bass','jeff','edu','office','doc','don',\n",
    "    'month','copy','name','comment','email','need','phone','point','thing','request','look','ben','michael','list',\n",
    "    'help','delainey','fax','morning','use','tomorrow','thank','phillip','hotmail','guy','robert','night','lon',\n",
    "    'part','talk','kate','home','mailto','person','address','form','jeffrey','something','end','line','hour',\n",
    "    'place','march','love','anything','paul','giron','smith','hope','darron','jim','kevin','weekend','george',\n",
    "    'north','someone','section','richard','discus','bob','jacoby','ena','room','see','demand','desk','area',\n",
    "    'everyone','greg','detail','jason','afternoon','discussion','tom','kslaw','check','basis','visit','mcconnell',\n",
    "    'miller','entity','location','peter','monday','response','show','page','jennifer','lot','meet','respond',\n",
    "    'yesterday','pdx','house','june','larry','jan','dan','city','july','judy','friday','julie','shirley','meter',\n",
    "    'level','fyi','addition','martin','anyone','generation','department','type','rick','friend','period','word',\n",
    "    'lisa','think','class','johnson','org','robin','thompson','columbiagas','didn','april','william','lee','thomas',\n",
    "    'hey','adam','stephen','man','sender','tim','taylor','organization','center','everything','ferc','notice',\n",
    "    'start','davis','york','sorry','cell','return','street','hernandez','thursday','campbell','care','content',\n",
    "    'curve','minute','floor','stinson','janet','head','move','kind','kent','tuesday','sheila','send','suzanne',\n",
    "    'brenda','kim','matter','fgt','carolyn','cindy','ccampbell','tell','fwd','crenshaw','baumbach','linda','side',\n",
    "    'clark','mind','hain','wharton','future','errol','carlos','hand','matt','bruce','gossett','brian','try',\n",
    "    'wednesday','calendar','laura','nothing','doug','llc','rebecca','rob','stephanie','austin','victor','join',\n",
    "    'joseph','couple', 'allen', 'kean', 'arnold', 'var', 'keith', 'lucy', 'grigsby','com', 'cc', 'pm', 'http',\n",
    "    'market','size', 'image', 'td', 'align', 'font','www', 'gas' ,'send', 'original', 'pt','tr','send','br','k','f','b' ))\n",
    "    exclude = set(string.punctuation) \n",
    "    lemma = WordNetLemmatizer()\n",
    "    \n",
    "    text=text.rstrip()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    stop_free = \" \".join([i for i in text.lower().split() if((i not in stop) and (not i.isdigit()))])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word, pos='v') for word in punc_free.split())\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning, removing stop words\n",
    "\n",
    "analysis_df=emails_df[['From', 'To', 'Date','content']].dropna().copy()\n",
    "analysis_df = analysis_df.loc[analysis_df['To'].map(len) == 1]\n",
    "analysis_df[\"clean_content\"]=analysis_df.content.apply(clean)\n",
    "analysis_df[\"clean_content\"].to_csv('clean_c.csv', index = False, header = True)\n",
    "analysis_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Data Before And After Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_simple = emails_df.content[0]\n",
    "print(content_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data After Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean(emails_df.content[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvector = TfidfVectorizer(analyzer='word', stop_words='english', max_df=0.4, min_df=5)\n",
    "short_analysis=analysis_df.sample(5000)\n",
    "wordvector_fit = wordvector.fit_transform(short_analysis.clean_content)\n",
    "feature = wordvector.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make elbow plot, find k\n",
    "distortions = []\n",
    "k_list = range(1, 5)\n",
    "for N in k_list:\n",
    "    clf = KMeans(n_clusters=N, \n",
    "            max_iter=50, \n",
    "            init='k-means++', \n",
    "            n_init=1)\n",
    "    labels = clf.fit_predict(wordvector_fit)\n",
    "    wordvector_fit_2d = wordvector_fit.todense()\n",
    "    distortions.append(sum(np.min(cdist(wordvector_fit_2d, clf.cluster_centers_, 'euclidean'), axis=1)) / wordvector_fit_2d.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(k_list, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering Algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N = 4\n",
    "clf = KMeans(n_clusters=N, \n",
    "            max_iter=50, \n",
    "            init='k-means++', \n",
    "            n_init=1)\n",
    "labels = clf.fit_predict(wordvector_fit)\n",
    "\n",
    "wordvector_fit_2d = wordvector_fit.todense()\n",
    "pca = PCA(n_components=2).fit(wordvector_fit_2d)\n",
    "datapoint = pca.transform(wordvector_fit_2d)\n",
    "\n",
    "label = [\"#e05f14\", \"#e0dc14\", \"#2fe014\", \"#14d2e0\"]\n",
    "color = [label[i] for i in labels]\n",
    "plt.scatter(datapoint[:, 0], datapoint[:, 1], c=color)\n",
    "\n",
    "centroids = clf.cluster_centers_\n",
    "centroidpoint = pca.transform(centroids)\n",
    "plt.scatter(centroidpoint[:, 0], centroidpoint[:, 1], marker='^', s=150, c='#000000')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA is useful here for us to find latent manifolds in text data. \n",
    "#### Here, we try to categorize all the data in to 10 topics, by choosing specific words and classifying them by comparing the difference between the weighs of those specific words. From the scores showing below, there are ten topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New LDA\n",
    "text_clean=[]\n",
    "for text in short_analysis['clean_content']:\n",
    "    text_clean.append(text.split())\n",
    "dictionary = corpora.Dictionary(text_clean)\n",
    "text_term_matrix = [dictionary.doc2bow(text) for text in text_clean]\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(text_term_matrix, num_topics=10, id2word = dictionary, passes=30)\n",
    "topics = ldamodel.print_topics(num_topics=10, num_words=10)\n",
    "for i, j in enumerate(topics):\n",
    "    print(\"Topic: {}\\n{}\\n\".format(i, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining these latent manifold is definitely a tool for us to improve our data mining becasue we can know how we will categorize the data, cleaning the data, and further more make predictions. By picking right topics here, our data can even be reduced so we can have better time complexity running the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixed Model (GMM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=4).fit(datapoint)\n",
    "labels = gmm.predict(datapoint)\n",
    "plt.scatter(datapoint[:, 0], datapoint[:, 1], c=labels, s=40, cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
